#ifndef TEXTNET_LAYER_WORD_REP_INPUT_LAYER_INL_HPP_
#define TEXTNET_LAYER_WORD_REP_INPUT_LAYER_INL_HPP_

#include <iostream>
#include <fstream>
#include <sstream>

#include <mshadow/tensor.h>
#include "../layer.h"
#include "../op.h"

namespace textnet {
namespace layer {

// this layer is originally designed for pretrain char LSTM
// it reads a word rep file generated by word2vec and output examples for pretrain char LSTM
template<typename xpu>
class WordRepInputLayer : public Layer<xpu>{
 public:
  WordRepInputLayer(LayerType type) { this->layer_type = type; }
  virtual ~WordRepInputLayer(void) {}
  
  virtual int BottomNodeNum() { return 0; }
  // x (the char sequence, end by a space token), y (word representation by word2vec)
  virtual int TopNodeNum() { return 2; }
  virtual int ParamNodeNum() { return 0; }

  virtual void Require() {
    // default value, just set the value you want
    this->defaults["shuffle_seed"] = SettingV(123);

    // require value, set to SettingV(),
    // it will force custom to set in config
    this->defaults["data_file"] = SettingV();
    this->defaults["batch_size"] = SettingV();
    this->defaults["max_word_len"] = SettingV();
    
    Layer<xpu>::Require();
  }

  virtual void SetupLayer(std::map<std::string, SettingV> &setting,
                          const std::vector<Node<xpu>*> &bottom,
                          const std::vector<Node<xpu>*> &top,
                          mshadow::Random<xpu> *prnd) {
    Layer<xpu>::SetupLayer(setting, bottom, top, prnd);
    
    utils::Check(bottom.size() == BottomNodeNum(), "WordRepInputLayer:bottom size problem."); 
    utils::Check(top.size() == TopNodeNum(), "WordRepInputLayer:top size problem.");
                  
    data_file    = setting["data_file"].s_val;
    batch_size   = setting["batch_size"].i_val;
    max_word_len = setting["max_word_len"].i_val;

    ReadWordRepData();
    
    line_ptr = 0;
    sampler.Seed(shuffle_seed);
  }

  // read the word2vec generated file (by Ofey)
  void ReadWordRepData() {
    utils::Printf("Open data file: %s\n", data_file.c_str());	
    std::ifstream fin(data_file.c_str());
    utils::Check(fin, "Open data file problem.");
    std::vector<std::string> lines;
    std::string s;
    while (!fin.eof()) {
      std::getline(fin, s);
      if (s.empty()) break;
      lines.push_back(s);
    }
    fin.close();
	utils::Printf("Line count in file: %d\n", lines.size());

    std::istringstream iss;
    iss.str(lines[0]); // the first line (word_vocab_size, feat_size)
    iss >> feat_size >> feat_size;
	utils::Printf("Wore rep size: %d\n", feat_size);

    word_rep_set.Resize(mshadow::Shape4(lines.size()-1, 1, 1, feat_size), 0);

    for (int i = 1; i < lines.size(); ++i) { // from the 2nd line
      iss.clear();
	  iss.seekg(0, iss.beg);
      iss.str(lines[i]);

      string word;
      iss >> word;
      word_set.push_back(word);

      int j = 0;
      while (!iss.eof()) {
        iss >> word_rep_set[word_set.size()-1][0][0][j++];
      }
      utils::Check(j == feat_size, "WordRepInputLayer: feat size error.");
    }

    // gen example ids
    for (int i = 0; i < word_set.size(); ++i) {
      example_ids.push_back(i);
    }
  }
  
  virtual void Reshape(const std::vector<Node<xpu>*> &bottom,
                       const std::vector<Node<xpu>*> &top,
					   bool show_info = false) {
    utils::Check(bottom.size() == BottomNodeNum(), "WordRepInputLayer:bottom size problem."); 
    utils::Check(top.size() == TopNodeNum(), "WordRepInputLayer:top size problem.");
    top[0]->Resize(batch_size, 1, 1, max_word_len+1, true);               // x
    top[1]->Resize(batch_size, 1, 1, feat_size,    true);                 // y
	if (show_info) {
      top[0]->PrintShape("top0");
      top[1]->PrintShape("top1");
	}
  }

  // end with space
  void conv_word_2_char_sequence(string word, vector<int> &char_seq) {
    char_seq.clear();
    for (int i = 0; i < word.size(); ++i) {
      int ascii = int(word[i]);
      utils::Check(ascii < 128, "WordRepInputLayer: only support characters less than 128");
      char_seq.push_back(ascii);
    }
    char_seq.push_back(32); // space
  }
  
  virtual void Forward(const std::vector<Node<xpu>*> &bottom,
                       const std::vector<Node<xpu>*> &top) {
    using namespace mshadow::expr;
    mshadow::Tensor<xpu, 4> x        = top[0]->data;
    mshadow::Tensor<xpu, 4> y        = top[1]->data;
    mshadow::Tensor<xpu, 2> x_length = top[0]->length;

    utils::Check(x.size(0) == batch_size, "ORC: error, need reshape.");
    x = -1.f, y = -10000.f, x_length = -1;
    
    for (int batch_idx = 0; batch_idx < batch_size; ++batch_idx) {
      if (this->phrase_type == kTrain && line_ptr == 0) {
        this->sampler.Shuffle(example_ids);
      }
      int example_id = example_ids[line_ptr];
      vector<int> char_seq;
      conv_word_2_char_sequence(word_set[example_id], char_seq);
      utils::Check(char_seq.size() < max_word_len+1, "WordRepInputLayer: word length error.");
      for (int i = 0; i < char_seq.size(); ++i) {
        x[batch_idx][0][0][i] = char_seq[i];
      }
      x_length[batch_idx][0] = char_seq.size();
      y[batch_idx][0][0] = F<op::identity>(word_rep_set[example_id][0][0]);
      
      line_ptr = (line_ptr + 1) % word_set.size();
    }
  }
  
  virtual void Backprop(const std::vector<Node<xpu>*> &bottom,
                        const std::vector<Node<xpu>*> &top) {
    using namespace mshadow::expr;
  }

 public:
  std::string data_file;
  int batch_size, max_word_len, feat_size;
  vector<string> word_set;
  mshadow::TensorContainer<xpu, 4> word_rep_set;
  std::vector<int> example_ids;
  int line_ptr, shuffle_seed;
  utils::RandomSampler sampler;
};
}  // namespace layer
}
#endif 

